[["index.html", "Data Science Showcase Chapter 1 Introduction", " Data Science Showcase Shaun Latham, Glen Arch 2023-11-30 Chapter 1 Introduction This is a concept webpage designed to showcase a portfolio of data science skills possessed the authors. The purpose is to provide prospective employers with a taster of our capabilities to supplement an application. Please take some time to explore the variety of interactive plots, dashboards and applications visualising the output of machine learning models and statistical tests we have prepared. Following redundancy from Pfizer, UK, we are both searching for new employment. If you are an employer and would like to discuss recruitment with us, please get in touch using the contact details found in the ‘about’ section or via the email addresses below. - Shaun Latham: Shaun_M_Latham@outlook.com - Glen Arch: glen.philip.arch@gmail.com About the authors: Shaun Latham Senior Scientist - Pfizer. Glen Arch Automation Scientist - Pfizer "],["unsupervised-learning.html", "Chapter 2 Unsupervised Learning 2.1 Introduction 2.2 Exploring the data 2.3 Analysis by unsupervised learning techniques 2.4 Predict a species", " Chapter 2 Unsupervised Learning 2.1 Introduction Welcome to the Data Science Showcase, in this section we are detailing unsupervised learning and R. Documentation relating to the decision making and time-management of the development of this code can be found here. As part of the wider project, we had decided to demonstrate competency with unsupervised learning techniques and with R. The team therefore decided to find an example dataset then tidy and model it using R. Actions within the team were tracked in the following issues: Sprint DSS-8: Create unsupervised learning model DSS-15: Find a compatible data-set ((ShaunLatham?) (GlenArch?)) DSS-23: Build &amp; train model - K-means ((GlenArch?)) DSS-56: Build &amp; train model - principal component analysis ((ShaunLatham?)) DSS-24: Visualize the data &amp; output of models ((ShaunLatham?) (GlenArch?)) 2.2 Exploring the data (ShaunLatham?) DSS-15: Find a compatible dataset. The team decided to utilize the iris dataset, which comes built into R. This dataset is a collection of measurements taken from individuals of three species of the Iris genus; I. setosa, I. versicolor and I. virginica, which are flowers. Below is a glimpse of the table and a summary of the population sizes and distributions: glimpse(iris) ## Rows: 150 ## Columns: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s… summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Firstly, it appears the variable names could use some tidying, the values are measured in cm but this is not reflected. The names are corrected below and saved to a new dataframe called ‘data’: data &lt;- iris %&gt;% rename(&quot;Petal length (cm)&quot; = Petal.Length, &quot;Petal width (cm)&quot; = Petal.Width, &quot;Sepal length (cm)&quot; = Sepal.Length, &quot;Sepal width (cm)&quot; = Sepal.Width) glimpse(data) ## Rows: 150 ## Columns: 5 ## $ `Sepal length (cm)` &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, … ## $ `Sepal width (cm)` &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, … ## $ `Petal length (cm)` &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, … ## $ `Petal width (cm)` &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, … ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, se… Clearly, from looking at the tables alone, it is not easy to visualize how each species differs for these variables. Below are distribution plots examining each of the numeric variables: fig1sub1 &lt;-ggplot(data, aes(x=`Petal length (cm)`, group=Species, fill = Species)) + geom_density(alpha=0.8) fig1sub2&lt;- ggplot(data, aes(x=`Petal width (cm)`, group=Species, fill = Species)) + geom_density(alpha=0.8, show.legend=FALSE) fig1sub3 &lt;- ggplot(data, aes(x=`Sepal length (cm)`, group=Species, fill = Species)) + geom_density(alpha=0.8, show.legend=FALSE) fig1sub4 &lt;- ggplot(data, aes(x=`Sepal width (cm)`, group=Species, fill = Species)) + geom_density(alpha=0.8, show.legend=FALSE) figure &lt;- ggarrange(fig1sub1, fig1sub2, fig1sub3, fig1sub4, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), ncol = 2, nrow = 2, common.legend = TRUE, legend = &quot;bottom&quot;) annotate_figure(figure, top = text_grob(&quot;Figure 1: Density of flower dimensions by species&quot;,face=&quot;bold&quot;,size=14)) Alternatively, the data can be visualized as boxplots, indicating which variables are significantly different between species: fig2sub1 &lt;- ggplot(data, aes(x=`Petal length (cm)`, y=Species, fill=Species)) + geom_boxplot() + geom_signif(comparisons = list(c(&quot;versicolor&quot;, &quot;virginica&quot;), c(&quot;setosa&quot;,&quot;versicolor&quot;)), map_signif_level=TRUE) fig2sub2 &lt;- ggplot(data, aes(x=`Petal width (cm)`, y=Species, fill=Species)) + geom_boxplot() + geom_signif(comparisons = list(c(&quot;versicolor&quot;, &quot;virginica&quot;), c(&quot;setosa&quot;,&quot;versicolor&quot;)), map_signif_level=TRUE) fig2sub3 &lt;- ggplot(data, aes(x=`Sepal length (cm)`, y=Species, fill=Species)) + geom_boxplot() + geom_signif(comparisons = list(c(&quot;versicolor&quot;, &quot;virginica&quot;), c(&quot;setosa&quot;,&quot;versicolor&quot;)), map_signif_level=TRUE) fig2sub4 &lt;- ggplot(data, aes(x=`Sepal width (cm)`, y=Species, fill=Species)) + geom_boxplot() + geom_signif(comparisons = list(c(&quot;versicolor&quot;, &quot;virginica&quot;), c(&quot;setosa&quot;,&quot;versicolor&quot;)), map_signif_level=TRUE) figure &lt;- ggarrange(fig2sub1, fig2sub2, fig2sub3, fig2sub4, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), ncol = 2, nrow = 2, common.legend = TRUE, legend = &quot;bottom&quot;) annotate_figure(figure, top = text_grob(&quot;Figure 2: Boxplots of flower dimensions by species&quot;,face=&quot;bold&quot;,size=14)) 2.3 Analysis by unsupervised learning techniques 2.3.1 3.1 Principal Component Analysis (ShaunLatham?) DSS-56: Build &amp; train model - principal component analysis: While PCA is normally used in situations where variance is contained across many variables, it can also be used to reduce the four dimensions of our Iris dataset to explain variance across a few eigenvectors through the data. Below, a covariance matrix is visualized and PCA is performed on the data; a summary of the proportion of variance explained by each eigenvector: corr_matrix &lt;- cor(data[,0:4]) ggcorrplot(corr_matrix) It can be seen from the Scree plot that 92.46% of variance in the dataset is explained in the fist principal component with an additional 5.30% from the second principal component. As such, the variance of the dataset can be effectively modelled in two dimensions rather than four. data.pca &lt;- prcomp(data[,0:4],scale=FALSE) summary(data.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 2.0563 0.49262 0.2797 0.15439 ## Proportion of Variance 0.9246 0.05307 0.0171 0.00521 ## Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 fig3sub1 &lt;- fviz_eig(data.pca, add_labels=TRUE) fig3sub2 &lt;- fviz_cos2(data.pca, choice = &quot;var&quot;, axes = 1:2) #fig3sub3 &lt;- fviz_pca(data.pca) figure3 &lt;- ggarrange(fig3sub1, fig3sub2, labels = c(&quot;A&quot;, &quot;B&quot;), ncol = 2, common.legend = TRUE, legend = &quot;bottom&quot;) figure3 #fig3sub3 Below is a plot of sample scores for PC1, PC2 &amp; PC3 coloured by species. It is clear that each species presents its own cluster, though there is some overlap between the I. versicolor and I. virginica clusters. fig4sub1 &lt;- autoplot(data.pca, data = data, colour=&quot;Species&quot;) fig4sub1 mycolors &lt;- c(&#39;orange&#39;, &#39;green&#39;, &#39;blue&#39;) data$color &lt;- mycolors[ as.numeric(data$Species) ] fig4sub2 &lt;- plot3d(data.pca$x[,1], xlab=&#39;PC1 (Rx^2 = 0.92)&#39;, data.pca$x[,2], ylab=&#39;PC2 (Rx^2 = 0.05)&#39;, data.pca$x[,3], zlab=&#39;PC3 (Rx^2 = 0.01)&#39;, col = data$color) #rglwidget() fig4sub2 2.3.2 K-Means Analysis (GlenArch?) DSS-23: Build &amp; train model - K-means: #Load the data into the dataframe df &lt;- iris #Remove any missing values df &lt;- na.omit(df) #Remove the species column from the dataset df_iris &lt;- df df &lt;- df[-c(5)] #Scale all of the values from 0 to 1 #df &lt;- scale(df) km &lt;- kmeans(df, 3, 100) km.pca &lt;- kmeans(data.pca$x[,1:2],3,100) fig5sub1 &lt;- fviz_nbclust(df, kmeans, nstart=2, method = &quot;wss&quot;)+labs(title=NULL) plot(fig5sub1) fig5sub2 &lt;- fviz_nbclust(df, kmeans, nstart = 2, method = &quot;silhouette&quot;)+labs(title=NULL) fig5sub2 fig5sub3 &lt;- fviz_nbclust(df, kmeans,nstart = 2,method = &quot;gap_stat&quot;)+labs(title=NULL) fig5sub3 fig6sub1 &lt;- fviz_cluster(km, df)+labs(title = &quot;K-Means: Raw Data&quot;) fig6sub2 &lt;- fviz_cluster(km.pca, df)+labs(title = &quot;K-Means: PCA Scores&quot;) figure6 &lt;- ggarrange(fig6sub1, fig6sub2, labels = c(&quot;A&quot;, &quot;B&quot;), ncol = 2, common.legend = TRUE, legend = &quot;bottom&quot;) figure6 cluster_species &lt;- km.pca$cluster cluster_species &lt;- recode(cluster_species, &quot;1&quot; = &quot;virginica&quot;, &quot;2&quot; = &quot;versicolor&quot;, &quot;3&quot; = &quot;setosa&quot;) data$cluster_species &lt;- cluster_species confusionMatrix(data = factor(data$Species), reference = factor(data$cluster_species)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 0 0 50 ## versicolor 47 3 0 ## virginica 14 36 0 ## ## Overall Statistics ## ## Accuracy : 0.02 ## 95% CI : (0.0041, 0.0573) ## No Information Rate : 0.4067 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : -0.47 ## ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 0.0000 0.07692 0.0000 ## Specificity 0.4382 0.57658 0.5000 ## Pos Pred Value 0.0000 0.06000 0.0000 ## Neg Pred Value 0.3900 0.64000 0.5000 ## Prevalence 0.4067 0.26000 0.3333 ## Detection Rate 0.0000 0.02000 0.0000 ## Detection Prevalence 0.3333 0.33333 0.3333 ## Balanced Accuracy 0.2191 0.32675 0.2500 2.4 Predict a species (ShaunLatham?) Having explored the data and analysed it by PCA and K-Means, the authors would like to demonstrate the predictive potential of these algorithms. While other supervised learning techniques would be more appropriate for this application, this chapter on unsupervised learning is the only chapter we intend to develop in R; therefore, this is the only opportunity to showcase the interactive R Shiny elements displayed below. Take some time to input some measurements of your own and see which species your flower is likely to be: knitr::include_url(&quot;https://shaun-m-latham.shinyapps.io/data_science_showcase_unsupervised_learning/&quot;, height=&quot;600px&quot;) #Function to predict cluster from unknown. kmeans_predictor &lt;- function(data,PetalLength,PetalWidth,SepalLength,SepalWidth){ test_data &lt;- array(c(PetalLength,PetalWidth,SepalLength,SepalWidth), dim=c(1,4)) model &lt;- kmeans(data[,1:4],3,nstart=25) prediction &lt;- cl_predict(model, test_data) return(prediction) } #App to input flower dimensions and predict species. shinyApp( ui = fluidPage( titlePanel(&quot;Predict Species&quot;), sidebarLayout( sidebarPanel( numericInput(&quot;PetalLength&quot;, &quot;Petal length (cm):&quot;, 0, min = 0, max = 10), numericInput(&quot;PetalWidth&quot;, &quot;Petal width (cm):&quot;, 0, min = 0, max = 10), numericInput(&quot;SepalLength&quot;, &quot;Sepal length (cm):&quot;, 0, min = 0, max = 10), numericInput(&quot;SepalWidth&quot;, &quot;Sepal width (cm):&quot;, 0, min = 0, max = 10), actionButton(&quot;submit&quot;,&quot;Submit&quot;)), mainPanel( textOutput(&quot;prediction&quot;), imageOutput(&#39;plot3&#39;)) )), server = function(input, output) { output$plot3 &lt;- renderImage({ # When input$n is 1, filename is ./images/image1.jpeg filename &lt;- normalizePath(file.path(&#39;./&#39;, paste(&#39;silly-dog&#39;, &#39;.png&#39;, sep=&#39;&#39;))) # Return a list containing the filename list(src = filename) }, deleteFile = FALSE) calculate &lt;- eventReactive(input$submit,{kmeans_predictor(data, input$PetalLength, input$PetalWidth, input$SepalLength, input$SepalWidth)}) output$prediction &lt;- renderText(calculate()) }, options = list(height = 500) ) Shiny applications not supported in static R Markdown documents "],["supervised-learning.html", "Chapter 3 Supervised Learning 3.1 Introduction", " Chapter 3 Supervised Learning (shaunLatham?), (GlenArch?) 3.1 Introduction This chapter aims to demonstrate competency with supervised learning techniques. The workflow is captured using Jira under the issue DSS-7: DSS-14 - Identify compatible datasets DSS-16 - Identify predictor and target variables. DSS-17 - Decide on algorithms and scoring metrics. DSS-18 - Tidy and clean the datasets. DSS-19 - Engineer features where appropriate. DSS-20 - Build and train models. DSS-21 - Evaluate and visualise models. It was decided to target the following techniques: Partial Least-Squares ((GlenArch?)) Linear / Quadratic Regression ((ShaunLatham?)) Support Vector Machines ((ShaunLatham?), (GlenArch?)) Decision Tree Classifiers ((GlenArch?)) Logistic Regression ((ShaunLatham?)) K-nearest neighbours ((ShaunLatham?), (GlenArch?)) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
